{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GeospatialFM.datasets.ssl4eo.utils import get_ssl4eo_metadata\n",
    "from GeospatialFM.data_process import pretrain_transform, multimodal_collate_fn\n",
    "from GeospatialFM.models import SpatialSpectralLowRankViTConfig, SpatialSpectralMAEViT\n",
    "from GeospatialFM.models import PositionalChannelEmbedding\n",
    "from GeospatialFM.datasets.ssl4eo import SSL4EODataset\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import json\n",
    "from safetensors import safe_open\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_vis_patch(patch, n_components=3):\n",
    "    # perform PCA on patch\n",
    "    pca = PCA(n_components=n_components)\n",
    "    B, L, D = patch.shape\n",
    "    patch_ = patch.reshape(B*L, D)\n",
    "    try:\n",
    "        patch_ = patch_.cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "    pca.fit(patch_)\n",
    "    patch_pca = pca.transform(patch_)\n",
    "\n",
    "    preprocessed_patches = patch_pca.reshape(B, int(L**0.5), int(L**0.5), 3)\n",
    "    return preprocessed_patches\n",
    "\n",
    "def norm_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSL4EO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_ssl4eo_metadata()\n",
    "optical_mean, optical_std = metadata[\"s2c\"][\"mean\"], metadata[\"s2c\"][\"std\"]\n",
    "radar_mean, radar_std = metadata[\"s1\"][\"mean\"], metadata[\"s1\"][\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict(train=SSL4EODataset(root=\"/home/haozhesi/Dropbox/GeospatialFM/data/geospatial/SSL4EO\"))\n",
    "transform = partial(pretrain_transform, optical_mean=optical_mean, optical_std=optical_std, radar_mean=radar_mean, radar_std=radar_std)\n",
    "collate_fn = partial(multimodal_collate_fn, transform=transform, random_crop=False, scale=1, crop_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define configuration\n",
    "config = SpatialSpectralLowRankViTConfig(\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    channel_embed_dims_per_head=1,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    decoder_embed_dim=512,\n",
    "    decoder_depth=4,\n",
    "    decoder_num_heads=16,\n",
    "    decoder_channel_embed_dims_per_head=1,\n",
    "    use_perception_field_mask=True,\n",
    "    attention_radius=640,\n",
    "    norm_pix_loss=False,\n",
    "    decoder_out_chans = 15,\n",
    "    pos_chan_embed_residual=True,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SpatialSpectralMAEViT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode\n",
    "device = 'cuda:0'\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "768*786*3*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "# model.load_state_dict(torch.load(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-ViT-Decoder-all-channels-decoder-depth-1/checkpoint-1500/pytorch_model.bin\", map_location=device))\n",
    "# model.load_state_dict(torch.load(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/archive/LRSSVIT-pos-res/checkpoint-500/pytorch_model.bin\", map_location=device))\n",
    "\n",
    "# Load the safetensors file\n",
    "# with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-chan-dim-4/checkpoint-5000/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "# with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT/checkpoint-22000/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "# with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-chan-dim-2-archived/checkpoint-15500/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "# with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-2-2-2-mse/checkpoint-4500/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "# with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-2-2-2-add-norm/checkpoint-2000/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "with safe_open(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LESSVIT_b1_d4/checkpoint-24600/model.safetensors\", framework=\"pt\", device=device) as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"Model loaded successfully from safetensors file.\")\n",
    "\n",
    "# model.load_state_dict(torch.load(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT-VITDecoder-no-channel-mask-depth-1/checkpoint-1500/pytorch_model.bin\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "optical = batch['optical'].to(device, dtype=torch.bfloat16)\n",
    "radar = batch['radar'].to(device, dtype=torch.bfloat16)\n",
    "optical_channel_wv = batch['optical_channel_wv']\n",
    "radar_channel_wv = batch['radar_channel_wv']\n",
    "spatial_resolution = batch['spatial_resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, cls_token, patch_tokens, hidden_states = model.encoder(optical=optical, radar=radar, optical_channel_wv=optical_channel_wv, radar_channel_wv=radar_channel_wv, \n",
    "                                                                    spatial_resolution=spatial_resolution, mask_ratio=0, channel_mask_ratio=0.)\n",
    "x, cls_token, patch_tokens = x.to(torch.float32).detach().cpu(), cls_token.to(torch.float32).detach().cpu(), patch_tokens.to(torch.float32).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_patches = []\n",
    "for hidden_state in hidden_states:\n",
    "    hidden_state = hidden_state.to(torch.float32)\n",
    "    # patches = hidden_state[:, 1:, 1:].mean(dim=1)\n",
    "    patches = hidden_state[:, 0, 1:]\n",
    "    # patches = hidden_state[:, 1, 1:]\n",
    "    patches = pca_vis_patch(patches.numpy())\n",
    "    pca_patches.append(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "vis_idx = 6\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = i*4 + j\n",
    "        if index == 0:\n",
    "            axs[i, j].imshow(norm_image(optical[vis_idx].permute(1, 2, 0).to(torch.float32).detach().cpu().numpy())[..., [3,2,1]])\n",
    "            axs[i, j].set_title('Original Patches')\n",
    "            axs[i, j].axis('off')\n",
    "        else:\n",
    "            index -= 1\n",
    "            axs[i, j].imshow(norm_image(pca_patches[index][vis_idx]))\n",
    "            axs[i, j].set_title(f'PCA at {index}')\n",
    "            axs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first and the last PCA\n",
    "vis_idx = 6\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(norm_image(optical[vis_idx].permute(1, 2, 0).to(torch.float32).detach().cpu().numpy())[..., [3,2,1]])\n",
    "axs[0].set_title('Input Image')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(norm_image(pca_patches[-1][vis_idx]))\n",
    "axs[1].set_title('Feature PCA')\n",
    "axs[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "with torch.no_grad():\n",
    "    output = model(optical=optical, radar=radar, optical_channel_wv=optical_channel_wv, radar_channel_wv=radar_channel_wv, spatial_resolution=spatial_resolution, mask_ratio=0.75, channel_mask_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_idx = 4\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# all images are in RGB format\n",
    "target_img = model.decoder.unpatchify(output['target'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0)\n",
    "optical_img = model.decoder.unpatchify(output['optical_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0)\n",
    "radar_img = model.decoder.unpatchify(output['radar_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0)\n",
    "multi_img = model.decoder.unpatchify(output['multi_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0)\n",
    "\n",
    "# # normalize the images to be in [0, 1]\n",
    "# target_img = norm_image(target_img)\n",
    "# optical_img = norm_image(optical_img)\n",
    "# radar_img = norm_image(radar_img)\n",
    "# multi_img = norm_image(multi_img)\n",
    "\n",
    "axs[0].imshow(target_img)\n",
    "axs[0].set_title('Target')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(optical_img)\n",
    "axs[1].set_title('Optical Reconstruction')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(radar_img)\n",
    "axs[2].set_title('Radar Reconstruction')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(multi_img)\n",
    "axs[3].set_title('Multi-modal Reconstruction')\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the spatial masks\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "n_patches = int(math.sqrt(output['optical_pos_mask'].shape[1]))\n",
    "\n",
    "axs[0].imshow(output['optical_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[0].set_title('Optical Positional Mask')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(output['radar_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[1].set_title('Radar Positional Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(output['multi_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[2].set_title('Multi-modal Positional Mask')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['multi_channel_mask'].shape\n",
    "output['multi_pos_mask'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the reconstruction of every channel of every modality\n",
    "vis_idx = 4\n",
    "fig, axs = plt.subplots(4, 15, figsize=(20, 6))\n",
    "target_img = model.decoder.unpatchify(output['target'])[vis_idx].detach().cpu().to(torch.float32).numpy()\n",
    "for i in range(15):\n",
    "    axs[0, i].imshow(target_img[i])\n",
    "    axs[0, i].axis('off')\n",
    "for j, modal in enumerate(['optical', 'radar', 'multi']):\n",
    "    spatial_mask = output[f'{modal}_pos_mask']\n",
    "    masked_patches = output[f'{modal}_recon'] * spatial_mask.unsqueeze(1).unsqueeze(-1).expand_as(output[f'{modal}_recon'])\n",
    "    unmasked_patches = output['target'] * (1-spatial_mask).unsqueeze(1).unsqueeze(-1).expand_as(output['target'])\n",
    "    \n",
    "    masked_channel_img = model.decoder.unpatchify(output[f'{modal}_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()\n",
    "    masked_img = model.decoder.unpatchify(masked_patches)[vis_idx].detach().cpu().to(torch.float32).numpy()\n",
    "    unmasked_img = model.decoder.unpatchify(unmasked_patches)[vis_idx].detach().cpu().to(torch.float32).numpy()\n",
    "    masked_pos_img = unmasked_img + masked_img\n",
    "    \n",
    "    channel_mask = output[f'{modal}_channel_mask'][vis_idx].detach().cpu().to(torch.bool).numpy()\n",
    "    for i in range(15):\n",
    "        if channel_mask[i]:\n",
    "            axs[j+1, i].imshow(masked_channel_img[i])\n",
    "            axs[j+1, i].axis('off')\n",
    "            axs[j+1, i].set_title('masked')\n",
    "        else:\n",
    "            # axs[j+1, i].imshow(masked_pos_img[i])\n",
    "            axs[j+1, i].imshow(masked_channel_img[i])\n",
    "            # axs[j+1, i].imshow(masked_img[i]) \n",
    "            axs[j+1, i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pca_patches = []\n",
    "for hidden_state in output['multi_hidden_states']:\n",
    "    hidden_state = hidden_state.to(torch.float32).detach().cpu()\n",
    "    # patches = hidden_state[:, 1:, 1:].mean(dim=1)\n",
    "    # patches = hidden_state[:, 2, 1:]\n",
    "    patches = hidden_state[:, 0, 1:]\n",
    "    # patches = hidden_state[:, 1:]\n",
    "    patches = pca_vis_patch(patches.numpy())\n",
    "    decoder_pca_patches.append(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "vis_idx = 6\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        index = i*4 + j\n",
    "        if index == 0:\n",
    "            axs[i, j].imshow(norm_image(optical[vis_idx].permute(1, 2, 0).to(torch.float32).detach().cpu().numpy())[..., [3,2,1]])\n",
    "            axs[i, j].set_title('Original Patches')\n",
    "            axs[i, j].axis('off') \n",
    "        elif index == 2:\n",
    "            axs[i, j].imshow(norm_image(decoder_pca_patches[1][0]))\n",
    "            axs[i, j].set_title('Positional Embedding')\n",
    "            axs[i, j].axis('off')\n",
    "        else:\n",
    "            index -= 1\n",
    "            axs[i, j].imshow(norm_image(decoder_pca_patches[index][vis_idx]))\n",
    "            axs[i, j].set_title(f'PCA at {index}')\n",
    "            axs[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
