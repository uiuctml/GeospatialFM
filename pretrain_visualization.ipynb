{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GeospatialFM.datasets.utils import get_ssl4eo_metadata\n",
    "from GeospatialFM.data import apply_transforms, pretrain_transform, multimodal_collate_fn\n",
    "from GeospatialFM.models import SpatialSpectralLowRankViTConfig, SpatialSpectralMAEViT\n",
    "from GeospatialFM.models.low_rank_attention import get_perception_field_mask\n",
    "from GeospatialFM.models import PositionalChannelEmbedding\n",
    "from GeospatialFM.datasets import SSL4EODataset\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception Field Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "num_patches = 196\n",
    "perception_field_mask = get_perception_field_mask(num_patches, patch_size, 10, attention_radius=640, cls_token=False)\n",
    "perception_field_mask.shape\n",
    "plt.imshow(perception_field_mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_perception = perception_field_mask[0].reshape(int(math.sqrt(perception_field_mask.shape[1])), int(math.sqrt(perception_field_mask.shape[1]))).numpy()\n",
    "idx = torch.rand(num_patches)\n",
    "keep_idx = torch.argsort(idx)[:int(num_patches*0.25)]\n",
    "random_mask = torch.zeros(num_patches)\n",
    "random_mask[keep_idx] = 1\n",
    "random_mask = random_mask.reshape(int(math.sqrt(num_patches)), int(math.sqrt(num_patches)))\n",
    "fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "ax[0].imshow(random_mask.numpy())\n",
    "ax[0].set_title(\"Random MAE Mask\")\n",
    "ax[1].imshow(one_perception, vmin=0, vmax=1)\n",
    "ax[1].set_title(\"One Perception Field Mask\")\n",
    "ax[2].imshow(random_mask.numpy() * one_perception)\n",
    "ax[2].set_title(\"Actual MAE Mask\")\n",
    "plt.show()\n",
    "# print(perception_field_mask[0].reshape(int(math.sqrt(perception_field_mask.shape[1])), int(math.sqrt(perception_field_mask.shape[1]))).numpy().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply random mask to perception field mask\n",
    "random_mask_idx = random_mask.reshape(-1).nonzero().squeeze()\n",
    "perception_field_mask_masked = perception_field_mask[random_mask_idx][:, random_mask_idx]\n",
    "plt.imshow(perception_field_mask_masked.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalChannel Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chan_embed = PositionalChannelEmbedding(embed_dim=768)\n",
    "tokens = torch.randn(1, 5, 196, 768)\n",
    "channel_ids = torch.arange(5).unsqueeze(0) * 1000\n",
    "pos_embed = pos_chan_embed(tokens, 10, channel_ids, cls_token=False).squeeze(0)\n",
    "print(pos_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the embedding of the first channel\n",
    "plt.imshow(torch.cat([pos_embed[i] for i in range(5)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSL4EO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_ssl4eo_metadata()\n",
    "optical_mean, optical_std = metadata[\"s2c\"][\"mean\"], metadata[\"s2c\"][\"std\"]\n",
    "radar_mean, radar_std = metadata[\"s1\"][\"mean\"], metadata[\"s1\"][\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"GeospatialFM/datasets/ssl4eo\", data_dir=\"/home/haozhesi/Dropbox/GeospatialFM/data/geospatial/SSL4EO\", keep_in_memory=False)\n",
    "dataset = dict(train=SSL4EODataset(root=\"/home/haozhesi/Dropbox/GeospatialFM/data/geospatial/SSL4EO\"))\n",
    "apply_transform = partial(apply_transforms, optical_mean=optical_mean, optical_std=optical_std, radar_mean=radar_mean, radar_std=radar_std)\n",
    "# dataset = dataset.map(apply_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_img(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "data_idx = np.random.randint(0, len(dataset['train']))\n",
    "sample_optical_image = torch.tensor(dataset['train'][data_idx]['optical']).numpy()\n",
    "sample_radar_image = torch.tensor(dataset['train'][data_idx]['radar']).numpy()\n",
    "sample_image = np.concatenate([sample_optical_image, sample_radar_image], axis=0)\n",
    "bands = metadata[\"s2c\"][\"bands\"] + metadata[\"s1\"][\"bands\"]\n",
    "rgb_image = sample_optical_image[[3, 2, 1]].transpose(1, 2, 0)\n",
    "rgb_image = norm_img(rgb_image)\n",
    "\n",
    "# visualize a data sample\n",
    "mpl.rcParams['axes.titlesize'] = 28\n",
    "mpl.rcParams['axes.labelsize'] = 24\n",
    "mpl.rcParams['xtick.labelsize'] = 18\n",
    "mpl.rcParams['ytick.labelsize'] = 18\n",
    "\n",
    "# Number of images excluding the RGB image\n",
    "num_images = sample_image.shape[0]\n",
    "# pick the 75 quantile location of each channel\n",
    "random_locs = []\n",
    "target_labels = []\n",
    "\n",
    "for i in [0, 0.25, 0.5, 0.75, 1]:\n",
    "    quantile = int(sample_image[1].size*i) if i != 1 else -1\n",
    "    loc = np.unravel_index(np.argsort(sample_image[1].flatten())[quantile], sample_image[1].shape)\n",
    "    random_locs.append(loc)\n",
    "    target_labels.append(f\"{i*100}%\")\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(32, 9))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a GridSpec with 3 rows and 8 columns\n",
    "gs = GridSpec(3, 11, figure=fig)\n",
    "\n",
    "ax_img = fig.add_subplot(gs[:, :3])\n",
    "ax_img.imshow(rgb_image)\n",
    "ax_img.axis('off')\n",
    "ax_img.set_title('RGB Image')\n",
    "for loc in random_locs:\n",
    "    ax_img.plot(loc[1], loc[0], 'x', markersize=10, markeredgewidth=4)\n",
    "\n",
    "# The rest of the images (adjust the range according to your number of images)\n",
    "for i in range(num_images):\n",
    "    # Calculate the position for the current image\n",
    "    row = i // 5\n",
    "    col = i % 5 \n",
    "    ax = fig.add_subplot(gs[row, col+3])\n",
    "    ax.imshow(sample_image[i])  # Replace with each individual image array\n",
    "    # plot cross at random locations\n",
    "    for loc in random_locs:\n",
    "        ax.plot(loc[1], loc[0], 'x', markersize=10, markeredgewidth=4)\n",
    "    # bold the title\n",
    "    ax.set_title(f\"{bands[i]}\")  # Replace with the title for each image\n",
    "    ax.axis('off')\n",
    "\n",
    "ax_curve = fig.add_subplot(gs[:, -3:])\n",
    "for loc, label in zip(random_locs, target_labels):\n",
    "    ax_curve.plot(np.arange(len(bands)), sample_image[:, loc[0], loc[1]], linewidth=3, marker='o', markersize=6, label=label)\n",
    "# ax_curve.legend(loc='upper right', fontsize=18)\n",
    "# ax_curve.set_xlabel('Bands')\n",
    "ax_curve.set_ylabel('Normalized Reflectance')\n",
    "ax_curve.set_title('Spectral Signature at Marked Locations')\n",
    "ax_curve.set_yticks([])\n",
    "# set the band names as xticks\n",
    "ax_curve.set_xticks(np.arange(len(bands)))\n",
    "ax_curve.set_xticklabels(bands, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# save as pdf\n",
    "# plt.savefig('spectral_signature.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader with Image Size Changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "collate_fn = partial(multimodal_collate_fn, transform=pretrain_transform, normalization=apply_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# for n, batch in enumerate(train_loader):\n",
    "#     print(batch['optical'].shape)\n",
    "#     if n == 4:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define configuration\n",
    "config = SpatialSpectralLowRankViTConfig(\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    decoder_embed_dim=512,\n",
    "    decoder_depth=8,\n",
    "    decoder_num_heads=16,\n",
    "    use_perception_field_mask=True,\n",
    "    attention_radius=640,\n",
    "    norm_pix_loss=False,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SpatialSpectralMAEViT(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of parameters\n",
    "num_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode\n",
    "model.to('cuda:0', dtype=torch.bfloat16)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # load checkpoint\n",
    "# model.load_state_dict(torch.load(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT/checkpoint-1000/pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(train_loader):\n",
    "    optical = batch['optical'].to('cuda:0', dtype=torch.bfloat16)\n",
    "    radar = batch['radar'].to('cuda:0', dtype=torch.bfloat16)\n",
    "    optical_channel_wv = batch['optical_channel_wv']\n",
    "    radar_channel_wv = batch['radar_channel_wv']\n",
    "    spatial_resolution = batch['spatial_resolution']\n",
    "    # Run forward pass\n",
    "    output = model(\n",
    "        optical=optical,\n",
    "        radar=radar,\n",
    "        optical_channel_wv=optical_channel_wv,\n",
    "        radar_channel_wv=radar_channel_wv,\n",
    "        mask_ratio=0.75,\n",
    "        channel_mask_ratio=0.5,\n",
    "        spatial_resolution=spatial_resolution,\n",
    "    )\n",
    "    break\n",
    "\n",
    "# Check output\n",
    "expected_keys = [\n",
    "    'target',\n",
    "    'optical_channel_mask', 'optical_recon', 'optical_pos_mask',\n",
    "    'radar_channel_mask', 'radar_recon', 'radar_pos_mask',\n",
    "    'multi_channel_mask', 'multi_recon', 'multi_pos_mask'\n",
    "]\n",
    "\n",
    "for key in expected_keys:\n",
    "    assert key in output, f\"Missing key in output: {key}\"\n",
    "    assert isinstance(output[key], torch.Tensor), f\"Output {key} is not a tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually compute the loss\n",
    "loss = {}\n",
    "total_loss = 0\n",
    "target = output['target']\n",
    "for modal in ['optical', 'radar', 'multi']:\n",
    "    if f'{modal}_recon' in output:\n",
    "        recon = output[f'{modal}_recon']\n",
    "        channel_mask = output[f'{modal}_channel_mask']\n",
    "        pos_mask = output[f'{modal}_pos_mask']\n",
    "        # positional MSE\n",
    "        pos_loss = (torch.mean((recon - target) ** 2, dim=[1, 3]) * pos_mask).sum() / pos_mask.sum()\n",
    "        print(\"pos_loss\", pos_loss, pos_mask.sum(), target.shape, recon.shape, pos_mask.shape)\n",
    "        # channel MSE\n",
    "        channel_loss = (torch.mean((recon - target) ** 2, dim=[2, 3]) * channel_mask).sum() / channel_mask.sum()\n",
    "        print(\"channel_loss\", channel_loss, channel_mask.sum(), target.shape, recon.shape, channel_mask.shape)\n",
    "        loss[f\"{modal}_pos_loss\"] = pos_loss\n",
    "        loss[f\"{modal}_channel_loss\"] = channel_loss\n",
    "        loss[f\"{modal}_loss\"] = pos_loss + channel_loss\n",
    "        total_loss += loss[f\"{modal}_loss\"]\n",
    "loss['total_loss'] = total_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_idx = 0\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axs[0].imshow(model.decoder.unpatchify(output['target'][:, :13])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0))\n",
    "axs[0].set_title('Target')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(model.decoder.unpatchify(output['optical_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0))\n",
    "axs[1].set_title('Optical Reconstruction')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(model.decoder.unpatchify(output['radar_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0))\n",
    "axs[2].set_title('Radar Reconstruction')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(model.decoder.unpatchify(output['multi_recon'])[vis_idx].detach().cpu().to(torch.float32).numpy()[[3,2,1]].transpose(1,2,0))\n",
    "axs[3].set_title('Multi-modal Reconstruction')\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the spatial masks\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "n_patches = int(math.sqrt(output['optical_pos_mask'].shape[1]))\n",
    "\n",
    "axs[0].imshow(output['optical_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[0].set_title('Optical Positional Mask')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(output['radar_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[1].set_title('Radar Positional Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(output['multi_pos_mask'][vis_idx].detach().cpu().to(torch.float32).numpy().reshape(n_patches, n_patches))\n",
    "axs[2].set_title('Multi-modal Positional Mask')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optical_recon = output['optical_recon']\n",
    "optical_recon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((target - optical_recon) ** 2, dim=[1, 3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optical_pos_mask = output['optical_pos_mask']\n",
    "optical_channel_mask = output['optical_channel_mask']\n",
    "optical_pos_mask.shape, optical_pos_mask.sum(), optical_channel_mask.shape, optical_channel_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(recon - target) ** 2 * pos_mask.unsqueeze(1).unsqueeze(-1).expand_as(recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((target - optical_recon) ** 2, dim=[2, 3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
