TRAINER:
  # Training related
  num_train_epochs: 100
  per_device_train_batch_size: 512
  per_device_eval_batch_size: 512
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  weight_decay: 0.01
  # dataloader
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_drop_last: false
  # optimizer
  optim: adamw_torch
  lr_scheduler_type: cosine
  # Model saving & evaluation
  save_strategy: epoch
  evaluation_strategy: epoch
  load_best_model_at_end: true
  metric_for_best_model: accuracy
  # Logging
  logging_dir: "./results/logs"
  logging_strategy: steps
  logging_steps: 10
  report_to: wandb
  # Checkpoints & Output
  output_dir: "./results/models"
  save_total_limit: 5
  overwrite_output_dir: false
  # Other
  seed: 0
  fp16: true
  push_to_hub: false
LOGGER:
  project: "GeoFoundation"
  entity: "ehzoahis"
  dir: "./results/"
DATASET:
  root: ./data/geospatial
  name: ''
  kwargs:
    bands: all
  train_transforms:
    crop_size: 224
    hflip_prob: 0.5
    normalize: false
  eval_transforms:
    crop_size: 224
    resize_size: 256
    normalize: false
  eval_metric: ''
  use_train_transform: true
MODEL:
  name: ''
  load_encoder: ''
  in_features: .
  out_features: .
  lp: false