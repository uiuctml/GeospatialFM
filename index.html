<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data2">
  <meta name="keywords" content="GFM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/render-scene.svg"> -->
  <link rel="icon" href="./static/logo/geospatial_logo.png", type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- <style>
    .hero-section {
      background-image: url('static/images/nasa-yZygONrUBe8-unsplash.jpg'); /* Adjust path */
      background-size: cover; /* Ensures the image covers the whole section */
      background-position: center;
      background-repeat: no-repeat;
      width: 100vw; /* Full viewport width */
      height: 50vh; /* Full viewport height */
      display: flex;
      align-items: center;
      justify-content: center;
      text-align: center;
    }
  </style> -->
  
  <style>
    blockquote footer {
      font-style: normal;
      text-align: right;
      margin-top: 5px;
      display: block;
    }

    .logo {
      width: 45px;   /* try smaller values like 50px or 60px */
      height: auto;  /* auto keeps aspect ratio */
      cursor: pointer;
    }

    /* .navbar-dropdown .navbar-item {
      text-align: center;
      display: block;
    } */
  </style>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" style="display: flex; align-items: center; gap: 8px;">
          <img src="./static/logo/geospatial_logo.png" alt="My Logo" class="logo" style="width: auto; height: auto;">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/uiuctml">
            About Us
          </a>
          <a class="navbar-item" href="https://github.com/uiuctml/SpatialTemporalGFM">
            Temporal-GFM (Coming Soon)
          </a>
          <a class="navbar-item" href="https://huggingface.co/GFM-Bench">
            GFM-Bench
          </a>
          <!-- <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape -->
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data
          </h1>
          <!-- <div class="title-section">
            <h1 class="title is-1 publication-title">
              Towards Foundation Model for Multi-modal and Hyperspectral Geospatial Data
            </h1>
          </div> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ehzoahis.github.io/">Haozhe Si</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Yuxuan Wan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://minhdo.ece.illinois.edu/">Minh Do</a><sup>1</sup>,
            </span> 
            <span class="author-block">
              <a href="https://deepakv.web.illinois.edu/">Deepak Vasisht</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://hanzhaoml.github.io/">Han Zhao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Hendrik F. Hamann</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign,</span>
            <span class="author-block"><sup>2</sup>IBM Research</span>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution,</span>
            <span class="author-block"><sup>&dagger;</sup>Corresponding author</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.12843"
                  class="external-link button is-normal is-rounded is-dark"
                  target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.12843"
                  class="external-link button is-normal is-rounded is-dark"
                  target="_blank">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/uiuctml/GeospatialFM"
                  class="external-link button is-normal is-rounded is-dark"
                  target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/GFM-Bench"
                  class="external-link button is-normal is-rounded is-dark"
                  target="_blank">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" width="20">
                  </span>
                  <span>Benchmark</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/hero_teaser.svg"
      alt="Pipeline of our proposed framework."/>
      
      <h2 class="subtitle has-text-centered">
        <!-- <span class="dnerf">PAC-Net</span> turns selfie videos from your phone into -->
        Our proposed framework <strong>Hyer-MAE</strong>.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="video_demo_0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="video_demo_1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="video_demo_0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="video_demo_1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Geospatial raster (imagery) data holds immense potential for enbaling extensive diverse high-impace downstream applications by providing spatial, temporal, and spectral information across multiple channels (e.g., spectral bands, polarizations) and sensing modalities. Recent work adapting existing self-supervised learning (SSL) approaches for geospatial data, they fall shorts of tailored model architectures and training objects. To address these limitations and better take advantage of geospatial data, we introduce <a href="#less-vit">LESS ViT</a>, a Vision Transformer architecture variant specifically deisgned for hyperspectral geospatial data, and Hyper-MAE, a Masked Auto Encoder based pre-training framework that employs a <a href="#less-vit">LESS ViT</a> encoder-decoder architecture and incorporates decoupled spatial and spectral masking to create a more challenging self-supervised pre-training objective. To evaluate empirical performance, we construct a benchmark, <a href="#gfm-bench">GFM-Bench</a>, which serves as a comprehensive benchmark over geospatial data. Experimental results demonstrate that our proposed method surpasses current state-of-the-art multi-modal geospatial foundation models, achieving superior performance with less computation and fewer parameters. The flexibility and extensibility of our framework make it a promising solution for future geospatial data analysis tasks that involve a wide range of modalities and channels.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video poster="" id="video_demo_0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_0.mp4"
                    type="video/mp4">
          </video> -->
          <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
        <!-- </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section id="less-vit" class="section">
  <div class="container is-max-desktop">
    <!-- PAC-Net. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Low-rank Efficient Spatial-Spectral ViT</h2>
        <div class="content has-text-justified">
          <p>
            Our <strong>Low-rank Efficient Spatial-Spectral (LESS) ViT</strong> architecture consists of three key components: (1) <a href="#patch-embed">Hyperspectral Patch Embedding Block</a>, (2) <a href="#attention-block">LESS Attention Block</a>, and (3) <a href="#mask">Perception Field Mask</a>.
          </p>
        </div>

        <h3 id="patch-embed" class="title is-4">Hyperspectral Patch Embedding</h3>
        <div class="content has-text-justified">
          <p>
            Hyperspectral images can contain tens to thousands of channels, distinguishing them from natural images that typically have three (RGB) channels. The rich spectral information encoded in these channels exhibits strong physical correlations that must be effectively leveraged. To exploit these spectral dependencies in subsequent attention blocks, we adopt a Tied Patch Embedding Layer that maintains spectral fidelity by explicitly embedding each channel information, and incorporate a continuous positional-channel embedding to capture both spatial and spectral relationships.
          </p>
        </div>
        <!-- <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="./static/images/hpe.svg"
            alt="PatchEmbed."/>
            <p>Hyperspectral Patch Embedding.</p>
          </div>
        </div> -->

        <div class="columns is-vcentered">
          <!-- First Image -->
          <div class="column has-text-centered">
            <img src="./static/images/hpe.svg"
                 alt="PatchEmbed."
                 style="width: 100%; height: auto;"/>
            <p>Hyperspectral Patch Embedding Block</p>
          </div>
        
          <!-- Second Image -->
          <div class="column has-text-centered">
            <img src="./static/images/less_vit.svg"
                 alt="LESSViT."
                 style="width: 50%; height: auto;"/>
            <p>LESS&nbsp;ViT</p>
          </div>
        </div>

        <h3 id="attention-block" class="title is-4">LESS Attention Block</h3>
        <div class="content has-text-justified">
          <p>
            Given the computation inefficiency of applying standard attention mechanism on spatial-spectral tokens, we address this limitation by proposing LESS attention block specifically deisgned for spatial-spectral tokens. To efficiently model spatial-spectral interactions, our LESS attention block approximates the full spatial-spectral attention matrix using a Kronecker product of separate spatial and spectral attention matrices. This approximation makes the attention block more scalable to larger numbers of channels while still capturing spatial-spectral interactions.
          </p>
        </div>
        <!-- <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="./static/images/less_vit.svg"
            alt="LESSViT."
            style="width: 40%; height: auto;"/>
            <p>LESSViT.</p>
          </div>
        </div> -->

        <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="static/images/perception_field_mask_visualization.svg"
              alt="PerceptionMask."
              style="width: 100%; height: auto;"/>
            <p>Perception Field Mask</p>
          </div>
        </div>

        <h3 id="mask" class="title is-4">Perception Field Mask</h3>
        <div class="content has-text-justified">
          <blockquote>
            <p>
              <em>"Everything is related to everything else, but near things are more related than distant things."</em>
            </p>
            <footer>— Tobler, First Law of Geography</footer>
          </blockquote>
          <p>
            To explicitly model spatial autocorrelation in geospatial data, we introduce the Perception Field Mask. The Perception Field Mask constrains the spatial attention computation by allowing each token to attend only to patches within a specified distance threshold. We define this threshold in meters rather than pixels, ensuring consistent spatial relationships across different image resolutions. This distance-based masking mechanism offers two key advantages: (1) it enforces locality in the attention computation, aligning with Tobler’s law, and (2) it enables the model to process images of varying sizes without downsampling, as the attention field remains spatially consistent regardless of resolution.
          </p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ PAC-Net. -->
    
    <!-- Dataset. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 id="gfm-bench" class="title">GFM-Bench</h2>

        <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="static/images/gfm-bench.png"
              alt="GFM-Bench"
              style="width: 100%; height: auto;"/>
            <p>GFM-Bench</p>
          </div>
        </div>

        <p>
          To encourage the usage of consistent evaluation protocol, we introduce <strong>GFM-Bench</strong>, a benchmark implemented using the HuggingFace framework for ease of use and providing standardized evaluation protocols. The current version of GFM-Bench consists three classification tasks (EuroSAT, BigEarthNet,and So2Sat) and four segmentation tasks (SegMunich, DFC2020, MARIDA, NLCD-L).
        </p>
        <br>
        <p>
          For more detailed information about GFM-Bench, please refer to our <a href="https://huggingface.co/GFM-Bench">GFM-Bench</a> page.
        </p>
      </div>
    </div>
    
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <h3 class="title is-4">EuroSat</h3>
          <p class="has-text-justified">
            EuroSAT is a land cover and land use classification dataset containing 13 MSI bands. It consists of 16,200 training samples, 5,400 test samples and 5,400 validation sample. Images of EuroSAT are 64 x 64 pixels.
          </p>
        </div>
      </div>
    </div> -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">BigEarthNet</h3>
        <div class="columns is-centered">
          <div class="column content">
            <p class="has-text-justified">
              BigEarthNet is a multi-label classification dataset consisting of 12 MSI bands and 2 SAR bands. The complete training set, validation set and test set we use consist of 269,695 samples, 123,723 samples and 125,866 samples respectively, and we only use 10% of the samples for fine-tuning and 10% of the samples for validation. Notice that we adopt the dataset from TorchGeo, which provides a different data split from the one used in previous works (354,196 training samples and 118,065 test samples). All images in the BigEarthNet dataset are 120 x 120 pixels.
            </p>
          </div>
        </div>
      </div>
    </div> -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <h3 class="title is-4">So2Sat</h3>
          <p class="has-text-justified">
            So2Sat is a local climate zone (LCZ) classification task consisting of 10 MSI bands. We reserve 10% of the training set as the validation set, resulting in 31,713 training samples, 3,523 validation samples and 48,307 test samples. Same as BigEarthNet, we use 10% of the complete training set and validation set during fine-tuning. Images are 32 x 32 pixels.
          </p>
        </div>
      </div>
    </div> -->

    <!-- <br/> -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <h3 class="title is-4">SegMunich</h3>
          <p class="has-text-justified">
            SegMunich is a land use and land cover segmentation dataset consisting of 10 MSI bands. This is a new Semantic Segmentation dataset collected by [36]. Images of SegMunich are 128 x 128 pixels.
          </p>
        </div>
      </div>
    </div> -->

    <!-- <br/> -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <h3 class="title is-4">DFC2020</h3>
          <p class="has-text-justified">
            DFC2020 is a dataset that includes 13 MSI bands along with VV and VH polarization channels. The original DFC2020 dataset contains only a validation set of 986 samples and a test set of 5,128 samples. To better utilize this dataset, we treat the original test set (5,128 samples) as our training and validation sets, and the original validation set (986 samples) as our test set. In addition, since the image resolution is 256 x 256 pixels, we follow CROMA’s method, further dividing each image of 256 ˆ 256 pixels into 9 smaller patches of 96 x 96 pixels with the overlap of 16 pixels. As a result, our final training set contains 41,537 training samples, the final validation set contains 4,615 samples and the final test set consists of 8,874 samples. All images are 96 x 96 pixels.
          </p>
        </div>
      </div>
    </div> -->

    <!-- <br/> -->

    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <h3 class="title is-4">MARIDA</h3>
          <p class="has-text-justified">
            Mobile Depth is a real-world DFD dataset captured by a Samsung Galaxy S3 mobile phone. It consists of 11 aligned focal stacks with 14 to 33 images per stack. Since neither depth ground-truth nor camera parameters are provided, we only perform qualitative evaluation and comparison on this dataset with no further finetuning.
          </p>
        </div>
      </div>
    </div> -->

    <br/>

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Quantitative Results</h2>

        <!-- Results on Synthetic Data. -->
        <h3 class="title is-4">Hyperspectral Optical Experiments</h3>
        <div class="content has-text-justified">
          <p>
            Our LESS ViT is pretrained on the SSL4EO-S12 dataset, which is a large-scale multi-modal geospatial dataset containing spatially and temporally aligned MSI with 13 channels and SAR imagery with 2 channels. We compare LESS ViT-Base with SoTA geospatial representation learning methods on our GFM-Bench.
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="static/images/main_table.png"
                  alt="Quantitative results for synthetic data"
                  style="width: 100%; height: auto;"/>
            <p>Quantitative results on seven datasets in the GFM-Bench under Fine-tuning (FT) and Linear Probing (LP).
            </p>
          </div>
        </div>
        <br/>
        <!--/ Results on Synthetic Data. -->

        <!-- Results on Real-shot Data. -->
        <h3 class="title is-4">Cross-Satellite Generalization</h3>
        <div class="content has-text-justified">
          <p>
            To also demonstrate the flexibility of our LESS ViT architecture in handling satellites with varying channels counts without architecture modifications, we evaluate our model on the NLCD-L dataset from GFM-Bench and compare our model against two baseline architectures. We also analyze computational efficiency by comparing encoder parameter counts, floating point operations (FLOPs) during fine-tuning, and both fine-tuning and inference latency on NLCD-L (20 channels) and BigEarthNet (12 channels). Performance is reported as mIoU for NLCD-L and mAP for BigEarthNet. To enable direct comparison, we normalize both FLOPs and wall-clock times relative to LESS ViT's baseline measurements.
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="./static/images/results2.png" width="600"
                  alt="PCA-Vis"
                  style="width: 100%; height: auto;"/>
            <p>
              Cross Satellite Generalization to Landsat and Model Efficiency.
            </p>
          </div>
        </div>

      </div>
    </div>
      
    <br/>

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Evaluation</h2>

        <!-- Results on Real-shot Data. -->
        <h3 class="title is-4">Visualization</h3>
        <div class="content has-text-justified">
          <p>
            We also perform principal component analysis (PCA) on the extracted patch features of each modal input. The top three principal components are visualized using three distinct colors. We interpolate the resulting image back to the original input size for better interpretation.
          </p>
        </div>
        <div class="columns is-vcentered">
          <div class="column has-text-centered">
            <img src="./static/images/pcas.svg" width="600"
                 alt="PCA-Vis"
                 style="width: 100%; height: auto;"/>
            <p>
              Visualization of top PCA components on BigEarthNet.
            </p>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            There are three key observations. First, the multi-modal patch features are mainly influenced by optical features, which is expected given the higher number of optical channels compared to radar channels. Second, for both the optical and multi-modal patch features, the principal components of similar land cover types have consistent color patterns. For instance, in optical feature visualizations, water bodies (Row 1) are highlighted in green, woodlands (Row 2) are represented by purple, and farmlands (Row 3) are primarily marked in yellow. Third, despite the presence of salt-and-pepper noise, the radar patch features demonstrate a correspondence between surfaces with similar textures. Smooth surfaces (water) tend to be marked in pink, while rough surfaces (woodlands and farmlands) are generally highlighted in green.
          </p>
        </div>

      </div>
    </div>

    </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{si2023fully,
      title={Fully Self-Supervised Depth Estimation from Defocus Clue},
      author={Si, Haozhe and Zhao, Bin and Wang, Dong and Gao, Yupeng and Chen, Mulin and Wang, Zhigang and Li, Xuelong},
      journal={arXiv preprint arXiv:2303.10752},
      year={2023}
    }
</code></pre>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{si2025scalablefoundationmodelmultimodal,
      title={Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data}, 
      author={Haozhe Si and Yuxuan Wan and Minh Do and Deepak Vasisht and Han Zhao and Hendrik F. Hamann},
      year={2025},
      eprint={2503.12843},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.12843}, 
    }   
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2503.12843"
         target="_blank">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" 
         href="https://github.com/uiuctml/GeospatialFM" 
         class="external-link" 
         disabled 
         target="_blank">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a 
            <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/"
              target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
          <p>
            This website is borrowed from the 
            <a
              href="https://github.com/nerfies/nerfies.github.ioM" 
              target="_blank">source code
            </a> of this website,
            of the Nerfies website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
