{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GeospatialFM.datasets.ssl4eo.utils import get_ssl4eo_metadata\n",
    "from GeospatialFM.data_process import apply_normalization, pretrain_transform, multimodal_collate_fn\n",
    "from GeospatialFM.models import SpatialSpectralLowRankViTConfig, SpatialSpectralMAEViT\n",
    "from GeospatialFM.models.low_rank_attention import get_perception_field_mask\n",
    "from GeospatialFM.models import PositionalChannelEmbedding\n",
    "from GeospatialFM.datasets.ssl4eo import SSL4EODataset\n",
    "from GeospatialFM.scripts.trainer import MAETrainer\n",
    "\n",
    "from functools import partial\n",
    "from accelerate import Accelerator\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception Field Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "num_patches = 64\n",
    "perception_field_mask = get_perception_field_mask(num_patches, patch_size, 10, attention_radius=640, cls_token=False)\n",
    "perception_field_mask.shape\n",
    "plt.imshow(perception_field_mask.numpy())\n",
    "# plt.axis('off')\n",
    "plt.xticks([-0.5, num_patches-0.5], labels=[0, num_patches])\n",
    "plt.yticks([-0.5, num_patches-0.5], labels=[0, num_patches])\n",
    "# plt.title(\"Attention Mask Example: 196x196\", fontsize=10, fontweight='bold')\n",
    "# plt.show()\n",
    "plt.savefig(\"perception_field_attention_mask.pdf\", bbox_inches='tight', pad_inches=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_perception = perception_field_mask[0].reshape(int(math.sqrt(perception_field_mask.shape[1])), int(math.sqrt(perception_field_mask.shape[1]))).numpy()\n",
    "idx = torch.rand(num_patches)\n",
    "keep_idx = torch.argsort(idx)[:int(num_patches*0.25)]\n",
    "random_mask = torch.zeros(num_patches)\n",
    "random_mask[keep_idx] = 1\n",
    "random_mask = random_mask.reshape(int(math.sqrt(num_patches)), int(math.sqrt(num_patches)))\n",
    "fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "ax[0].imshow(random_mask.numpy())\n",
    "ax[0].set_title(\"Random MAE Mask\")\n",
    "ax[1].imshow(one_perception, vmin=0, vmax=1)\n",
    "ax[1].set_title(\"One Perception Field Mask\")\n",
    "ax[2].imshow(random_mask.numpy() * one_perception)\n",
    "ax[2].set_title(\"Actual MAE Mask\")\n",
    "plt.show()\n",
    "# print(perception_field_mask[0].reshape(int(math.sqrt(perception_field_mask.shape[1])), int(math.sqrt(perception_field_mask.shape[1]))).numpy().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply random mask to perception field mask\n",
    "random_mask_idx = random_mask.reshape(-1).nonzero().squeeze()\n",
    "perception_field_mask_masked = perception_field_mask[random_mask_idx][:, random_mask_idx]\n",
    "plt.imshow(perception_field_mask_masked.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalChannel Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chan_embed = PositionalChannelEmbedding(embed_dim=768)\n",
    "tokens = torch.randn(1, 5, 64, 768)\n",
    "# channel_ids = torch.arange(5).unsqueeze(0) * 1000\n",
    "channel_ids = torch.tensor([492.4, 559.8, 664.6, 1613.7, 2202.4]).unsqueeze(0)\n",
    "pos_embed = pos_chan_embed(tokens, 10, channel_ids, cls_token=False).squeeze(0)\n",
    "print(pos_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the embedding of the first channel\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(torch.cat([pos_embed[i] for i in range(5)], dim=0), cmap=\"inferno\")\n",
    "# add tick label to y axis\n",
    "plt.yticks(np.arange(11)*32-0.5, labels=[f\"$\\lambda$ = {float(channel_ids[0, i//2]):.1f} $\\mu m$\" if i % 2 == 1 else \"\" for i in range(11)], fontsize=12, fontweight='bold')\n",
    "# \n",
    "plt.xticks(np.array([0, 96, 192, 288, 384, 480, 576, 672, 768])-0.5, labels=[0, 96, 192, 288, 384, 480, 576, 672, 768], fontsize=12)\n",
    "# plt.axis('off')\n",
    "plt.xlabel(\"Embedding Dimension\", fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"positional_channel_embedding.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSL4EO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_ssl4eo_metadata()\n",
    "optical_mean, optical_std = metadata[\"s2c\"][\"mean\"], metadata[\"s2c\"][\"std\"]\n",
    "radar_mean, radar_std = metadata[\"s1\"][\"mean\"], metadata[\"s1\"][\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"GeospatialFM/datasets/ssl4eo\", data_dir=\"/home/haozhesi/Dropbox/GeospatialFM/data/geospatial/SSL4EO\", keep_in_memory=False)\n",
    "dataset = dict(train=SSL4EODataset(root=\"/home/haozhesi/Dropbox/GeospatialFM/data/geospatial/SSL4EO\"))\n",
    "apply_transform = partial(apply_normalization, optical_mean=optical_mean, optical_std=optical_std, radar_mean=radar_mean, radar_std=radar_std)\n",
    "# dataset = dataset.map(apply_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_img(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "data_idx = np.random.randint(0, len(dataset['train']))\n",
    "data_idx = 117967\n",
    "# print(data_idx)\n",
    "sample = apply_transform(dataset['train'][data_idx])\n",
    "sample_optical_image = torch.tensor(sample['optical']).numpy()\n",
    "sample_radar_image = torch.tensor(sample['radar']).numpy()\n",
    "sample_image = np.concatenate([sample_optical_image, sample_radar_image], axis=0)\n",
    "bands = metadata[\"s2c\"][\"bands\"] + metadata[\"s1\"][\"bands\"]\n",
    "rgb_image = sample_optical_image[[3, 2, 1]].transpose(1, 2, 0)\n",
    "rgb_image = norm_img(rgb_image)\n",
    "\n",
    "# visualize a data sample\n",
    "# mpl.rcParams['axes.titlesize'] = 28\n",
    "# mpl.rcParams['axes.labelsize'] = 24\n",
    "# mpl.rcParams['xtick.labelsize'] = 18\n",
    "# mpl.rcParams['ytick.labelsize'] = 18\n",
    "\n",
    "# Number of images excluding the RGB image\n",
    "num_images = sample_image.shape[0]\n",
    "# pick the 75 quantile location of each channel\n",
    "random_locs = []\n",
    "target_labels = []\n",
    "\n",
    "for i in [0, 0.25, 0.5, 0.75, 1]:\n",
    "    quantile = int(sample_image[1].size*i) if i != 1 else -1\n",
    "    loc = np.unravel_index(np.argsort(sample_image[1].flatten())[quantile], sample_image[1].shape)\n",
    "    random_locs.append(loc)\n",
    "    target_labels.append(f\"{i*100}%\")\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(32, 9))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a GridSpec with 3 rows and 8 columns\n",
    "gs = GridSpec(3, 11, figure=fig)\n",
    "\n",
    "ax_img = fig.add_subplot(gs[:, :3])\n",
    "ax_img.imshow(rgb_image)\n",
    "ax_img.axis('off')\n",
    "ax_img.set_title('RGB Image')\n",
    "for loc in random_locs:\n",
    "    ax_img.plot(loc[1], loc[0], 'x', markersize=10, markeredgewidth=4)\n",
    "\n",
    "# The rest of the images (adjust the range according to your number of images)\n",
    "for i in range(num_images):\n",
    "    # Calculate the position for the current image\n",
    "    row = i // 5\n",
    "    col = i % 5 \n",
    "    ax = fig.add_subplot(gs[row, col+3])\n",
    "    ax.imshow(sample_image[i])  # Replace with each individual image array\n",
    "    # plot cross at random locations\n",
    "    for loc in random_locs:\n",
    "        ax.plot(loc[1], loc[0], 'x', markersize=10, markeredgewidth=4)\n",
    "    # bold the title\n",
    "    ax.set_title(f\"{bands[i]}\")  # Replace with the title for each image\n",
    "    ax.axis('off')\n",
    "\n",
    "ax_curve = fig.add_subplot(gs[:, -3:])\n",
    "for loc, label in zip(random_locs, target_labels):\n",
    "    ax_curve.plot(np.arange(len(bands)), sample_image[:, loc[0], loc[1]], linewidth=3, marker='o', markersize=6, label=label)\n",
    "# ax_curve.legend(loc='upper right', fontsize=18)\n",
    "# ax_curve.set_xlabel('Bands')\n",
    "ax_curve.set_ylabel('Normalized Reflectance')\n",
    "ax_curve.set_title('Spectral Signature at Marked Locations')\n",
    "ax_curve.set_yticks([])\n",
    "# set the band names as xticks\n",
    "ax_curve.set_xticks(np.arange(len(bands)))\n",
    "ax_curve.set_xticklabels(bands, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# save as pdf\n",
    "# plt.savefig('spectral_signature.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"patches/masked\")\n",
    "# idx = torch.rand(16)\n",
    "# keep_idx = torch.argsort(idx)[:int(16*0.5)]\n",
    "# random_mask = torch.zeros(16)\n",
    "# random_mask[keep_idx] = 1\n",
    "\n",
    "# print(random_mask)\n",
    "# channel_idx = torch.rand(num_images)\n",
    "# keep_channel_idx = torch.argsort(channel_idx)[:int(num_images*0.5)]\n",
    "# randon_channel_mask = torch.zeros(num_images) \n",
    "# randon_channel_mask[keep_channel_idx] = 1\n",
    "\n",
    "# cut the sample image into 3x3 patches\n",
    "patch_size = 264//3\n",
    "channel_indices = [1, 2, 3, 4, 13, 14]\n",
    "\n",
    "fig, ax = plt.subplots(7, 7, figsize=(10, 10))\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i == 0 and j == 0:\n",
    "            # fill in yellowish red pure color\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.8, cmap='Oranges', vmin=0, vmax=1)\n",
    "        elif i == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.2, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        elif j == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.3, cmap='Greens', vmin=0, vmax=1)\n",
    "        else:\n",
    "            plot_i = i-1\n",
    "            plot_j = j-1\n",
    "            ax[j, i].imshow(sample_image[channel_indices[plot_i], plot_j//3*patch_size:(plot_j//3+1)*patch_size, plot_j%3*patch_size:(plot_j%3+1)*patch_size])\n",
    "        ax[j, i].axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"sample_patches.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 264//3\n",
    "channel_indices = [1, 2, 3, 4, 13, 14]\n",
    "masked_channel_indices = [1, 3, 4]\n",
    "masked_positions = [0, 1, 3, 5]\n",
    "print(masked_channel_indices, masked_positions)\n",
    "\n",
    "fig, ax = plt.subplots(7, 7, figsize=(10, 10))\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i == 0 and j == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.8, cmap='Oranges', vmin=0, vmax=1)\n",
    "        elif i == 0:\n",
    "            if j-1 in masked_positions:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.4, cmap='Greys', vmin=0, vmax=1)\n",
    "            else:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.2, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        elif j == 0:\n",
    "            if i-1 in masked_channel_indices:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.4, cmap='Greys', vmin=0, vmax=1)\n",
    "            else:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.3, cmap='Greens', vmin=0, vmax=1)\n",
    "        else:\n",
    "            plot_i = i-1\n",
    "            plot_j = j-1\n",
    "            if plot_i in masked_channel_indices or plot_j in masked_positions:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.4, cmap='Greys', vmin=0, vmax=1)\n",
    "            else:\n",
    "                ax[j, i].imshow(sample_image[channel_indices[plot_i], plot_j//3*patch_size:(plot_j//3+1)*patch_size, plot_j%3*patch_size:(plot_j%3+1)*patch_size])\n",
    "        ax[j, i].axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"masked_patches.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 264//3\n",
    "channel_indices = [1, 2, 3, 4, 13, 14]\n",
    "masked_channel_indices = [1, 3, 4]\n",
    "unmasked_channel_indices = [0, 2, 5]\n",
    "masked_positions = [0, 1, 3, 5]\n",
    "unmasked_positions = [2, 4]\n",
    "print(masked_channel_indices, masked_positions)\n",
    "\n",
    "fig, ax = plt.subplots(7, 7, figsize=(10, 10))\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i == 0 and j == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.8, cmap='Oranges', vmin=0, vmax=1)\n",
    "        elif i == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.2, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        elif j == 0:\n",
    "            ax[j, i].imshow(np.ones((patch_size, patch_size))*0.3, cmap='Greens', vmin=0, vmax=1)\n",
    "        else:\n",
    "            try:\n",
    "                plot_i = unmasked_channel_indices[i-1]\n",
    "                plot_j = unmasked_positions[j-1]\n",
    "                ax[j, i].imshow(sample_image[channel_indices[plot_i], plot_j//3*patch_size:(plot_j//3+1)*patch_size, plot_j%3*patch_size:(plot_j%3+1)*patch_size])\n",
    "            except:\n",
    "                ax[j, i].imshow(np.ones((patch_size, patch_size))*0.5, cmap='Greys', vmin=0, vmax=1)\n",
    "        ax[j, i].axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"masked_patches_2.pdf\", bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_and_visualization(image, image_size, patch_size=16, attention_radius=640, median_mask=0.55, alpha=0.3):\n",
    "    num_patches = (image_size // patch_size)**2\n",
    "    perception_field_mask = get_perception_field_mask(num_patches, patch_size, 10, attention_radius=attention_radius, cls_token=False)\n",
    "    vis_perception_mask = perception_field_mask[int(num_patches*median_mask)].reshape(int(math.sqrt(perception_field_mask.shape[1])), int(math.sqrt(perception_field_mask.shape[1]))).numpy()\n",
    "    vis_perception_mask = TF.resize(torch.tensor(vis_perception_mask).unsqueeze(0), size=(image_size, image_size), interpolation=TF.InterpolationMode.NEAREST).squeeze(0)\n",
    "    \n",
    "    image = image.copy()\n",
    "    image = TF.center_crop(torch.tensor(image).permute(2, 0, 1), output_size=(image_size, image_size)).permute(1, 2, 0).numpy()\n",
    "    applied_mask = (1-vis_perception_mask.unsqueeze(-1).numpy()) * (1-alpha) + (1-vis_perception_mask.unsqueeze(-1).numpy()) * image * alpha + (vis_perception_mask.unsqueeze(-1)).numpy()*image\n",
    "    target_patch_idx = int(median_mask*num_patches)\n",
    "    target_i = target_patch_idx // int(num_patches**0.5)\n",
    "    target_j = target_patch_idx % int(num_patches**0.5)\n",
    "    return perception_field_mask, applied_mask, target_i, target_j\n",
    "\n",
    "vis_perception_mask_256, applied_mask_256, target_i_256, target_j_256 = get_mask_and_visualization(rgb_image, 256)\n",
    "vis_perception_mask_224, applied_mask_224, target_i_224, target_j_224 = get_mask_and_visualization(rgb_image, 224)\n",
    "vis_perception_mask_192, applied_mask_192, target_i_192, target_j_192 = get_mask_and_visualization(rgb_image, 192)\n",
    "vis_perception_mask_128, applied_mask_128, target_i_128, target_j_128 = get_mask_and_visualization(rgb_image, 128)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))  # Adjusted to 2x4 layout\n",
    "\n",
    "ax[0, 0].imshow(applied_mask_256)\n",
    "ax[0, 0].add_patch(plt.Rectangle((target_j_256*16, target_i_256*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[0, 0].set_title(\"Image Size: 256x256\", fontsize=26, fontweight='bold')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "\n",
    "ax[0, 1].imshow(applied_mask_224)\n",
    "ax[0, 1].set_title(\"Image Size: 224x224\", fontsize=26, fontweight='bold')\n",
    "ax[0, 1].add_patch(plt.Rectangle((target_j_224*16, target_i_224*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[1, 0].imshow(applied_mask_192)\n",
    "ax[1, 0].set_title(\"Image Size: 192x192\", fontsize=26, fontweight='bold')\n",
    "ax[1, 0].add_patch(plt.Rectangle((target_j_192*16, target_i_192*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[1, 0].axis('off')\n",
    "\n",
    "ax[1, 1].imshow(applied_mask_128)\n",
    "ax[1, 1].set_title(\"Image Size: 128x128\", fontsize=28, fontweight='bold')\n",
    "ax[1, 1].add_patch(plt.Rectangle((target_j_128*16, target_i_128*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[1, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# title\n",
    "# plt.suptitle(\"Perception Field Attention Mask\", fontsize=18, fontweight='bold')\n",
    "# plt.savefig(\"perception_field_mask.pdf\", bbox_inches='tight', pad_inches=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))  # Adjusted to 2x4 layout\n",
    "\n",
    "ax[0, 0].imshow(vis_perception_mask_256)\n",
    "ax[0, 0].set_title(\"Patch Number: 256\", fontsize=26, fontweight='bold')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(applied_mask_256)\n",
    "ax[0, 1].add_patch(plt.Rectangle((target_j_256*16, target_i_256*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[0, 1].set_title(\"Image Size: 256x256\", fontsize=26, fontweight='bold')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(vis_perception_mask_224)\n",
    "ax[0, 2].set_title(\"Patch Number: 196\", fontsize=26, fontweight='bold')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "ax[0, 3].imshow(applied_mask_224)\n",
    "ax[0, 3].set_title(\"Image Size: 224x224\", fontsize=26, fontweight='bold')\n",
    "ax[0, 3].add_patch(plt.Rectangle((target_j_224*16, target_i_224*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[0, 3].axis('off')\n",
    "\n",
    "ax[1, 0].imshow(vis_perception_mask_192)\n",
    "ax[1, 0].set_title(\"Patch Number: 144\", fontsize=26, fontweight='bold')\n",
    "ax[1, 0].axis('off')\n",
    "\n",
    "ax[1, 1].imshow(applied_mask_192)\n",
    "ax[1, 1].set_title(\"Image Size: 192x192\", fontsize=26, fontweight='bold')\n",
    "ax[1, 1].add_patch(plt.Rectangle((target_j_192*16, target_i_192*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[1, 1].axis('off')\n",
    "\n",
    "ax[1, 2].imshow(vis_perception_mask_128)\n",
    "ax[1, 2].set_title(\"Patch Number: 64\", fontsize=26, fontweight='bold')\n",
    "ax[1, 2].axis('off')\n",
    "\n",
    "ax[1, 3].imshow(applied_mask_128)\n",
    "ax[1, 3].set_title(\"Image Size: 128x128\", fontsize=26, fontweight='bold')\n",
    "ax[1, 3].add_patch(plt.Rectangle((target_j_128*16, target_i_128*16), 16, 16, edgecolor='red', facecolor='none'))\n",
    "ax[1, 3].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"perception_field_mask_visualization.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(((1-vis_perception_mask_256.unsqueeze(-1).numpy()) * img_256 * 0.3))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader with Image Size Changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "collate_fn = partial(multimodal_collate_fn, transform=pretrain_transform, normalization=apply_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# for n, batch in enumerate(train_loader):\n",
    "#     print(batch['optical'].shape)\n",
    "#     if n == 4:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define configuration\n",
    "config = SpatialSpectralLowRankViTConfig(\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    decoder_embed_dim=512,\n",
    "    decoder_depth=8,\n",
    "    decoder_num_heads=16,\n",
    "    use_perception_field_mask=True,\n",
    "    attention_radius=640,\n",
    "    norm_pix_loss=False,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SpatialSpectralMAEViT(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of parameters\n",
    "num_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode\n",
    "model.to('cuda:0', dtype=torch.bfloat16)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # load checkpoint\n",
    "# model.load_state_dict(torch.load(\"/home/haozhesi/Dropbox/GeospatialFM/results/models/LRSSVIT/checkpoint-1000/pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(train_loader):\n",
    "    optical = batch['optical'].to('cuda:0', dtype=torch.bfloat16)\n",
    "    radar = batch['radar'].to('cuda:0', dtype=torch.bfloat16)\n",
    "    optical_channel_wv = batch['optical_channel_wv']\n",
    "    radar_channel_wv = batch['radar_channel_wv']\n",
    "    spatial_resolution = batch['spatial_resolution']\n",
    "    # Run forward pass\n",
    "    output = model(\n",
    "        optical=optical,\n",
    "        radar=radar,\n",
    "        optical_channel_wv=optical_channel_wv,\n",
    "        radar_channel_wv=radar_channel_wv,\n",
    "        mask_ratio=0.75,\n",
    "        channel_mask_ratio=0.5,\n",
    "        spatial_resolution=spatial_resolution,\n",
    "    )\n",
    "    break\n",
    "\n",
    "# Check output\n",
    "expected_keys = [\n",
    "    'target',\n",
    "    'optical_channel_mask', 'optical_recon', 'optical_pos_mask',\n",
    "    'radar_channel_mask', 'radar_recon', 'radar_pos_mask',\n",
    "    'multi_channel_mask', 'multi_recon', 'multi_pos_mask'\n",
    "]\n",
    "\n",
    "for key in expected_keys:\n",
    "    assert key in output, f\"Missing key in output: {key}\"\n",
    "    assert isinstance(output[key], torch.Tensor), f\"Output {key} is not a tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually compute the loss\n",
    "loss = {}\n",
    "total_loss = 0\n",
    "target = output['target']\n",
    "for modal in ['optical', 'radar', 'multi']:\n",
    "    if f'{modal}_recon' in output:\n",
    "        recon = output[f'{modal}_recon']\n",
    "        channel_mask = output[f'{modal}_channel_mask']\n",
    "        pos_mask = output[f'{modal}_pos_mask']\n",
    "        # positional MSE\n",
    "        pos_loss = (torch.mean((recon - target) ** 2, dim=[1, 3]) * pos_mask).sum() / pos_mask.sum()\n",
    "        # channel MSE\n",
    "        channel_loss = (torch.mean((recon - target) ** 2, dim=[2, 3]) * channel_mask).sum() / channel_mask.sum()\n",
    "        loss[f\"{modal}_pos_loss\"] = pos_loss\n",
    "        loss[f\"{modal}_channel_loss\"] = channel_loss\n",
    "        loss[f\"{modal}_loss\"] = pos_loss + channel_loss\n",
    "        total_loss += loss[f\"{modal}_loss\"]\n",
    "loss['total_loss'] = total_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
